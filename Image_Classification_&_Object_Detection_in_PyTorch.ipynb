{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification & Object Detection in PyTorch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5kQregEBexA"
      },
      "source": [
        "*   MLP & Deep Learning\n",
        "*   Classification & Regression\n",
        "*   ReLU & Softmax Activations\n",
        "*   Loss Functions & Backpropagation\n",
        "*   CNN, Image Classificaiton & Object Detection\n",
        "*   Frameworks & PyTorch:\n",
        " *   A NumPy replacement that uses the power of GPUs\n",
        " *   A flexible and fast deep learning research platform\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10org14yD0XG"
      },
      "source": [
        "# Autograds in PyTorch\n",
        "Torch module for automatically calculating gradients of tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC-K34HJBJTs"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.rand(5, 3) # Tensors\n",
        "y = torch.rand(5, 3)\n",
        "print(x + y) # Operations\n",
        "print(torch.add(x, y)) # Operations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sswZVfO0YXtr"
      },
      "source": [
        "w = torch.randn(2,2, requires_grad=True) # Like wights in our NN case\n",
        "w_square = w**2\n",
        "L = w_square.mean() # L=(w^2)/4, like loss in our NN case\n",
        "print(w)\n",
        "print(w_square)\n",
        "print(L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKE_XqNnDtc2"
      },
      "source": [
        "print(w_square.grad_fn) # grad_fn shows the function that generated this variable\n",
        "\n",
        "print(w.grad) # currently empty, d?/dw\n",
        "L.backward() # calculate the gradient for dL/dw (mean_power function)\n",
        "print(w.grad)\n",
        "\n",
        "# L=(w^2)/4, so dL/dw = 2*w/4 = w/2\n",
        "print(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPq9HiCHHdQM"
      },
      "source": [
        "# Training in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efn_m8oNg-O3"
      },
      "source": [
        "## Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibZY4G3aHcws"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# Define a transform to normalize the data, download it, and create a loader (generator)\n",
        "#transform = transforms.Compose([transforms.ToTensor(),\n",
        "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#                               ])\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.0], [0.5]),  # Mean & Standard Deviation\n",
        "                               ])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOTsmqKyhGm-"
      },
      "source": [
        "## Creating Model & Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE7BcOJWH8iU"
      },
      "source": [
        "torch.cuda.is_available()\n",
        "\n",
        "# Build a feed-forward network\n",
        "# image size = 28X28X1 = 784 pixels\n",
        "model = nn.Sequential(nn.Linear(784, 128), # number neruons in first layer = 128 \n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64), # number neruons in second layer = 64\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10), # number neruons in second layer = 10\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "#----------------------------------|--------------------|--------------------|------------------------\n",
        "\n",
        "# net_L1,N1   = w1,1   * p1        + w1,2   * p2        +         ...        + w1,784   * p784       \n",
        "# net_L1,N2   = w2,1   * p1        + w2,2   * p2        +         ...        + w2,784   * p784       \n",
        "# net_L1,N128 = w128,1 * p1        + w128,2 * p2        +         ...        + w128,784 * p784       \n",
        "\n",
        "#----------------------------------|--------------------|--------------------|------------------------\n",
        "\n",
        "# net_L2,N1   = w1,1   * net_L1,N1 + w1,2   * net_L1,N2 +         ...        + w1,128   * net_L1,N128\n",
        "# net_L2,N2   = w2,1   * net_L1,N1 + w2,2   * net_L1,N2 +         ...        + w2,128   * net_L1,N128\n",
        "# net_L2,N64  = w64,1  * net_L1,N1 + w64,2  * net_L1,N2 +         ...        + w64,128  * net_L1,N128\n",
        "\n",
        "#----------------------------------|--------------------|--------------------|------------------------\n",
        "\n",
        "# net_L3,N1   = w1,1   * net_L2,N1 + w1,2   * net_L2,N2 +         ...        + w1,64    * net_L2,N64\n",
        "# net_L3,N2   = w2,1   * net_L2,N1 + w2,2   * net_L2,N2 +         ...        + w2,64    * net_L2,N64\n",
        "# net_L3,N10  = w10,1  * net_L2,N1 + w10,2  * net_L2,N2 +         ...        + w10,64   * net_L2,N64\n",
        "\n",
        "#----------------------------------|--------------------|--------------------|------------------------\n",
        "\n",
        "# Define the loss (NLLLoss to LogSoftmax = CrossEntropyLoss to Softmax but its practical implementation is more efficient)\n",
        "loss_function = nn.NLLLoss() # recieves LogSoftmax and labels\n",
        "\n",
        "# Get our data\n",
        "images, labels = next(iter(trainloader)) # returns the next item from the iterator to the RAM\n",
        "images = images.view(images.shape[0], -1) # Flatten images to be batch_size X feature vector (not image)\n",
        "\n",
        "# Forward pass, get our log-probabilities\n",
        "logps = model(images)\n",
        "loss = loss_function(logps, labels) # Calculate the loss with the logps and the labels\n",
        "print(loss)\n",
        "\n",
        "# Print network output for the first image and calculate the sum\n",
        "import numpy as np\n",
        "print(torch.exp(logps[0]))\n",
        "print(torch.sum(torch.exp(logps[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI-yD-qDhMj2"
      },
      "source": [
        "## Calculate Gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlP8z7aYhPQO"
      },
      "source": [
        "# Build a feed-forward network\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# Read a batch of data\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1) # From shape torch.Size([64, 1, 28, 28]) to torch.Size([64, 784])\n",
        "\n",
        "# Forward pass and calcuate loss\n",
        "loss_function = nn.NLLLoss()\n",
        "logps = model(images) \n",
        "loss = loss_function(logps, labels)\n",
        "\n",
        "print('Gradient before backward pass: \\n', model[0].weight.grad) # d?/d(model[0].weight)\n",
        "\n",
        "loss.backward() # loss is function of model[0].weights, model[1].weights, and model[2].weights\n",
        "\n",
        "print('Gradient after backward pass: \\n', model[0].weight.grad) # d(loss)/d(model[0].weight) --> sparse in MNIST case of gray scale images\n",
        "print('Gradient after backward pass: \\n', model[0].weight.grad.nonzero()) # returns indices of non zero elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rilwrAArix4o"
      },
      "source": [
        "## Backpropagation Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1TOTitxi0Bw"
      },
      "source": [
        "# Training the network\n",
        "from torch import optim\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) # Requires the parameters to optimize and a learning rate\n",
        "\n",
        "# Read a batch of data\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1) # From shape torch.Size([64, 1, 28, 28]) to torch.Size([64, 784])\n",
        "\n",
        "# Clear the gradients because they are accumulated in PyTorch\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Forward pass and then calucate the gradients (gradients are requirment for for backward pass)\n",
        "output = model.forward(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('First layer weights gradient: ', model[0].weight.grad) # sparse in MNIST case of gray scale images\n",
        "\n",
        "# Take an update step and few the view weights\n",
        "print('Initial weights: ', model[0].weight)\n",
        "initial_weights = model[0].weight.detach().numpy().copy() # detach if variable has requires_grad=True\n",
        "optimizer.step() # The gradients are computed by the loss.backward() call\n",
        "print('Updated weights: ', model[0].weight)\n",
        "\n",
        "new_weights = model[0].weight\n",
        "index = new_weights.grad.nonzero()[0] # First changed weight\n",
        "print(\"-----------------------------\")\n",
        "print(\"The first weight in the first layer was \" + str(initial_weights[0,0]))\n",
        "print(\"The first weight in the first layer became \" + str(new_weights[0,0].detach().numpy()))\n",
        "# Most probably not changed due to zero gradient due to MNIST images black edges\n",
        "print(\"-----------------------------\")\n",
        "print(\"The first changed weight in the first layer was \" + str(initial_weights[index[0], index[1]]))\n",
        "print(\"The gradient of that weight = \" + str(model[0].weight.grad[index[0], index[1]]))\n",
        "print(\"The first changed weight in the first layer became \" + str(new_weights[index[0], index[1]].detach().numpy())) # This should be equal to previous value - lr * gradient calculated in previous line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0acQ3L2mHkd"
      },
      "source": [
        "## Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WhJTeNimEW9"
      },
      "source": [
        "# Training on data\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        images = images.view(images.shape[0], -1) # Flatten MNIST images into a 784 long vector\n",
        "    \n",
        "        # Training pass\n",
        "        optimizer. ...()\n",
        "        output = model.forward(images)\n",
        "        loss = loss_function(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer. ...()\n",
        "       \n",
        "        running_loss += loss.item() # item() converts the value into a plain python number living in the CPU RAM\n",
        "    else: # At the epoch ends\n",
        "        print(\"Average Batch training loss this epoch: \" + str({running_loss/len(trainloader)})) # Avergae over batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbgF9A0inU5Q"
      },
      "source": [
        "## Test the model on samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB37jj3vn8Gl"
      },
      "source": [
        "Run it more than one time to check!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zVef7NnYzn"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images, labels = next(iter(trainloader))\n",
        "img = images[0].view(1, 784)\n",
        "with torch.no_grad(): # Turn off gradients to speed up this part\n",
        "    logps = model.forward(img)\n",
        "\n",
        "# Output of the network are logits, need to take softmax for probabilities\n",
        "ps = torch.exp(logps).squeeze()\n",
        "plt.imshow(images[0].permute(1,2,0).squeeze()) # from torch.Size([1, 28, 28]) to torch.Size([28, 28, 1]) to torch.Size([28, 28])\n",
        "print(ps)\n",
        "print(sum(ps))\n",
        "\n",
        "prediction_label = np. ...(ps).item()\n",
        "conf = int(...(ps).item()*100)\n",
        "print(\"Our trained model predicts that image to be: \" + str(prediction_label) + \", with a confidence of \" + str(conf) + \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQtt2BSVo8IU"
      },
      "source": [
        "# Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwusQ7egp6p7"
      },
      "source": [
        "The problem wth the above network is that they lose spatial orientation.\n",
        "\n",
        "<img src=https://raw.githubusercontent.com/heshameraqi/Image-Classifiaciton-Object-Detection-with-PyTorch-Workshop/main/imgs/CNN_view.png width=\"500\">\n",
        "\n",
        "(Image adapted from: learnopencv.com, 11/2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2j2HDYswVVS"
      },
      "source": [
        "## Use a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_mAP_UYo-80"
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "\n",
        "dir(models)\n",
        "model = models.vgg16(pretrained=True)\n",
        "print(model)\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F69GzVrawdHK"
      },
      "source": [
        "## Classify an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kequP7kCpcZj"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "!wget -q -nc https://raw.githubusercontent.com/heshameraqi/Image-Classifiaciton-Object-Detection-with-PyTorch-Workshop/main/imgs/dog.jpg\n",
        "img = Image.open(\"dog.jpg\") # A yellow labrador (Image source: Wikipedia)\n",
        "plt.imshow(img)\n",
        "\n",
        "img_t = transform(img)\n",
        "batch_t = torch.unsqueeze(img_t, 0)\n",
        "print(batch_t.shape)\n",
        "\n",
        "model.eval() # Out our model in evaluation mode\n",
        "out = model(batch_t)\n",
        "print(out.shape)\n",
        "percentage = torch.nn.functional. ...(..., dim=1)[0] * 100\n",
        "\n",
        "!wget -q -nc https://raw.githubusercontent.com/heshameraqi/Image-Classifiaciton-Object-Detection-with-PyTorch-Workshop/main/data/imagenet1000.names\n",
        "classes_dict = eval(open('./imagenet1000.names').read())\n",
        "_, indices = torch.sort(out, descending=True)\n",
        "[(classes_dict[i.item()], percentage[i].item()) for i in indices[0][:5]] # Print top 5 classifications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coiLYThr03Ix"
      },
      "source": [
        "# R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkcbQDIsGAHq"
      },
      "source": [
        "## Run the detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XilSp1wsybaZ"
      },
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 50 # Depends on the GPU memory, the bigger the faster :)\n",
        "\n",
        "# Load model and transform\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.eval() # Use the model in evaluation mode\n",
        "transform = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "n_rows, n_cols = 256, 256 # Input size of the used model\n",
        "\n",
        "# Read input image and resize it\n",
        "size = 400, 400 # resize the input image\n",
        "!wget -q -nc https://raw.githubusercontent.com/heshameraqi/Image-Classifiaciton-Object-Detection-with-PyTorch-Workshop/main/imgs/cat.jpg\n",
        "img = Image.open(\"cat.jpg\") # A yellow labrador (Image source: Wikipedia)\n",
        "img.thumbnail(size, Image.ANTIALIAS)\n",
        "plt.imshow(img)\n",
        "\n",
        "# Configurations. Selected to suite the workshop limited time, normally they should provide much more boxes (exponential growth)\n",
        "window_min_size = 100  # object minimum dimension\n",
        "window_max_size = 200 # object minimum dimension\n",
        "window_size_step = 100\n",
        "window_pos_stride = 50\n",
        "\n",
        "# Crop images\n",
        "images_to_classify = []\n",
        "location_size = []\n",
        "for s_h in range(window_min_size, window_max_size, window_size_step):\n",
        "  for s_w in range(window_min_size, window_max_size, window_size_step):\n",
        "    for h in range(0, img.size[0], window_pos_stride):\n",
        "      for w in range(0, img.size[1], window_pos_stride):\n",
        "          # Crop image\n",
        "          height_to = np.min([h+s_h, n_rows])\n",
        "          width_to = np.min([w+s_w, n_cols])\n",
        "          # [h,w,height_to,width_to] = [110,50,290,130]  # For debugging\n",
        "          box = (w, h, w+width_to, h+height_to)\n",
        "          cropped_img = img.crop(box)\n",
        "\n",
        "          # Preprocess image\n",
        "          location_size.append([h,w,height_to,width_to])\n",
        "          images_to_classify.append(...(cropped_img))\n",
        "\n",
        "# Classify cropped images as batches\n",
        "confidences = []\n",
        "winner_classes_idx = []\n",
        "for i in tqdm(range(0, len(images_to_classify), batch_size)):\n",
        "  preds = model(torch.stack(images_to_classify[i:i+batch_size]))\n",
        "  for c in preds.detach().numpy():\n",
        "    confidences.append(np.max(c))\n",
        "    winner_classes_idx.append(np.argmax(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlZdVXwrC616"
      },
      "source": [
        "## Visualize Detected Objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABezxBSX5Osc"
      },
      "source": [
        "import matplotlib.patches as patches\n",
        "import random\n",
        "\n",
        "# Number of predictions needed\n",
        "predictions = 1\n",
        "\n",
        "# Read classes names\n",
        "!wget -q -nc https://raw.githubusercontent.com/heshameraqi/Image-Classifiaciton-Object-Detection-with-PyTorch-Workshop/main/data/imagenet1000.names\n",
        "classes_dict = eval(open('./imagenet1000.names').read())\n",
        "\n",
        "# Sort predictions\n",
        "sort_idx = (-np.array(confidences)).argsort()\n",
        "confidences = [confidences[i] for i in sort_idx]\n",
        "location_size = [location_size[i] for i in sort_idx]\n",
        "winner_classes_idx = [winner_classes_idx[i] for i in sort_idx]\n",
        "\n",
        "# Bounding-box colors (at most visualize 20 objects)\n",
        "plt.figure()\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(img)\n",
        "nbrObjects = min([len(confidences), predictions])\n",
        "cmap = plt.get_cmap(\"tab20b\")\n",
        "colors = [cmap(i) for i in np.linspace(0, 1, nbrObjects)]\n",
        "bbox_colors = random.sample(colors, nbrObjects)\n",
        "for i in range(nbrObjects):\n",
        "  if confidences[i] <= 0.85:\n",
        "    break\n",
        "  [h, w, height_to, width_to] = location_size[i]\n",
        "  box_w = width_to - w\n",
        "  box_h = height_to - h\n",
        "  # Create and add the bbox to the plot\n",
        "  ax.add_patch(patches.Rectangle((w, h), box_w, box_h, linewidth=2, edgecolor=bbox_colors[i], facecolor=\"none\"))\n",
        "  # Add label\n",
        "  plt.text(w, h, s=classes_dict[winner_classes_idx[i]], color=\"white\", verticalalignment=\"top\", bbox={\"color\": bbox_colors[i], \"pad\": 0})\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09WVU-rf6ptT"
      },
      "source": [
        "# Fast R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGXOkMh9IHZr"
      },
      "source": [
        "## Imports, read model, read image, and install Selective Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASrAwS5QIGu7"
      },
      "source": [
        "!pip install selectivesearch\n",
        "import selectivesearch\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "# Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 50\n",
        "\n",
        "# Load model and transform\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.eval() # Use the model in evaluation mode\n",
        "transform = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "# Read and resize image\n",
        "size = 400, 400 # resize the input image\n",
        "!wget -q -nc https://raw.githubusercontent.com/heshameraqi/Image-Classifiaciton-Object-Detection-with-PyTorch-Workshop/main/imgs/cat.jpg\n",
        "img = Image.open(\"cat.jpg\") # A yellow labrador (Image source: Wikipedia)\n",
        "img.thumbnail(size, Image.ANTIALIAS)\n",
        "plt.imshow(img)\n",
        "\n",
        "# Read classes names\n",
        "!wget -q -nc https://raw.githubusercontent.com/heshameraqi/Image-Classifiaciton-Object-Detection-with-PyTorch-Workshop/main/data/imagenet1000.names\n",
        "classes_dict = eval(open('./imagenet1000.names').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo8nSSy8GmZs"
      },
      "source": [
        "## Run the detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBkc7I-5Fa_1"
      },
      "source": [
        "img_lbl, regions = selectivesearch.selective_search(np.asarray(img), scale=200, sigma=0.4, min_size=200)\n",
        "candidates = set()\n",
        "for r in regions:\n",
        "    if r['rect'] in candidates: # excluding same rectangle (with different segments)\n",
        "        continue\n",
        "    if r['size'] < 20*20: # excluding regions smaller than X pixels\n",
        "        continue\n",
        "    ''''x, y, w, h = r['rect'] # excluding regions with extreme aspect ratios\n",
        "    if h==0 or w==0 or w / h > 3 or h / w > 3:\n",
        "        continue'''\n",
        "    candidates.add(r['rect'])\n",
        "candidates = list(candidates)\n",
        "\n",
        "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))\n",
        "ax.imshow(img)\n",
        "for x, y, w, h in candidates:\n",
        "    rect = patches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=1)\n",
        "    ax.add_patch(rect)\n",
        "plt.title('Selective Search Result has ' + str(len(candidates)) + ' boxes')\n",
        "plt.show()\n",
        "\n",
        "images_to_classify = []\n",
        "for x, y, w, h in candidates: # Candidates: bottom, left, width, height\n",
        "    box = (x, y, x+w, y+h)\n",
        "    cropped_img = img.crop(box)\n",
        "    images_to_classify.append(transform(cropped_img))\n",
        "\n",
        "confidences = []\n",
        "winner_classes_idx = []\n",
        "for i in tqdm(range(0, len(images_to_classify), batch_size)):\n",
        "  preds = ...(torch.stack(images_to_classify[i:i+batch_size]))\n",
        "  for c in preds.detach().numpy():\n",
        "    confidences.append(np.max(c))\n",
        "    winner_classes_idx.append(np.argmax(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgGx8zJmFhfM"
      },
      "source": [
        "## Visualize detected objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3GL_HmOFkNN"
      },
      "source": [
        "# Predictions needed\n",
        "predictions = 5\n",
        "\n",
        "# Sort predictions\n",
        "sort_idx = (-np.array(confidences)).argsort()\n",
        "confidences = [confidences[i] for i in sort_idx]\n",
        "candidates = [candidates[i] for i in sort_idx]\n",
        "\n",
        "plt.figure()\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(img)\n",
        "\n",
        "# Bounding-box colors (at most visualize 20 objects)\n",
        "nbrObjects = min([len(confidences), predictions])\n",
        "cmap = plt.get_cmap(\"tab20b\")\n",
        "colors = [cmap(i) for i in np.linspace(0, 1, nbrObjects)]\n",
        "bbox_colors = random.sample(colors, nbrObjects)\n",
        "for i in range(nbrObjects):\n",
        "  #if confidences[i] <= 0.85:\n",
        "  #  break\n",
        "  \n",
        "  [x, y, w, h] = candidates[i]\n",
        "  # Create and add the bbox to the plot\n",
        "  ax.add_patch(patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=bbox_colors[i], facecolor=\"none\"))\n",
        "  # Add label\n",
        "  plt.text(x, y, s=classes_dict[winner_classes_idx[i]],\n",
        "      color=\"white\", verticalalignment=\"top\", bbox={\"color\": bbox_colors[i], \"pad\": 0},)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTbIw7DP61gf"
      },
      "source": [
        "# Single-shot Object Detection (YOLOv3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3XXn_4X7Nq_"
      },
      "source": [
        "## Install PyTorch-YOLOv3 Python requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXXtUadr7OiQ"
      },
      "source": [
        "!git clone https://github.com/heshameraqi/Image-Classification-Object-Detection-with-PyTorch-Workshop.git\n",
        "%cd Image-Classification-Object-Detection-with-PyTorch-Workshop\n",
        "!mkdir weights\n",
        "%cd weights\n",
        "# wget https://pjreddie.com/media/files/yolov3-tiny.weights \n",
        "!wget https://pjreddie.com/media/files/yolov3.weights\n",
        "%cd ../..\n",
        "\n",
        "%cd Image-Classification-Object-Detection-with-PyTorch-Workshop/PyTorch-YOLOv3\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5h6tUsN7IYU"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChNhEqeR60Xj"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "from models_yolo import *\n",
        "from utils.utils import *\n",
        "from utils.datasets import *\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import lycon  # A minimal and fast image library for Python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s84cO7ip7KH5"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDmR422D6_PO"
      },
      "source": [
        "img_size = 416\n",
        "network_config_path = 'config/yolov3.cfg'  #yolov3.cfg or yolov3-tiny.cfg\n",
        "weights_path = '../weights/yolov3.weights'\n",
        "class_path = '../data/coco.names'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxXSU9D7Ykw"
      },
      "source": [
        "## Setup model, load weights, and load list of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q79Yf7OQ7Ay4"
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "model = Darknet(network_config_path, img_size=img_size)\n",
        "model.load_darknet_weights(weights_path)\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "model.eval() # Set to evaluation mode\n",
        "\n",
        "classes = load_classes(class_path)  # Extracts class labels from file\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tItRGFhv7aYH"
      },
      "source": [
        "## Load image and run the model on image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFjN8Jqc7dln"
      },
      "source": [
        "input_img_filename = '../imgs/cat.jpg'\n",
        "img = lycon.load(input_img_filename)\n",
        "\n",
        "# Image preprocessing\n",
        "h, w, _ = img.shape\n",
        "img, pad = pad_to_square(img, 127.5)\n",
        "padded_h, padded_w, _ = img.shape\n",
        "# Resize to target shape\n",
        "img = lycon.resize(img, height=img_size, width=img_size)\n",
        "# Channels-first and normalize\n",
        "img = torch.from_numpy(img).float().permute((2, 0, 1)) / 255.\n",
        "\n",
        "# Configure input image\n",
        "img = img[None, :, :, :]  # create a batch of 1 image\n",
        "img = Variable(img.type(Tensor))\n",
        "\n",
        "# Get detections\n",
        "with torch.no_grad():\n",
        "    detections = model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqHNJ7x87fiN"
      },
      "source": [
        "**Question: Why each detection shape is 10647 x 85? (MS COCO has 80 classes)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KPHBjzsXhcn"
      },
      "source": [
        "print(detections.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeMxhMVv7gwZ"
      },
      "source": [
        "## Visualize detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWhU9hpb7kPf"
      },
      "source": [
        "# Create plot\n",
        "img = np.array(Image.open(input_img_filename))\n",
        "plt.figure()\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(img)\n",
        "\n",
        "if detections is not None:\n",
        "  detections_resized = rescale_boxes(detections[0], img_size, img.shape[:2])\n",
        "  for detection in detections_resized[0:500]:  # just some of them\n",
        "    x1, y1, x2, y2, conf = detection[0:5]\n",
        "    # Create a Rectangle patch\n",
        "    box_w = x2 - x1\n",
        "    box_h = y2 - y1\n",
        "    bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=\"white\", facecolor=\"none\")\n",
        "    # Add the bbox to the plot\n",
        "    ax.add_patch(bbox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl9hsSm07l8P"
      },
      "source": [
        "## Using non-max suppresion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnX3h3As7m3m"
      },
      "source": [
        "# Object confidence threshold\n",
        "conf_th = 0.05 #@param { type: \"slider\", min:0, max:1, step:0.05 }\n",
        "\n",
        "# IoU thresshold for non-maximum suppression\n",
        "iou_th = 0.95 #@param { type: \"slider\", min:0, max:1, step:0.05 }\n",
        "\n",
        "input_img_filename = '../imgs/cat.jpg'\n",
        "img = lycon.load(input_img_filename)\n",
        "\n",
        "# Image preprocessing\n",
        "h, w, _ = img.shape\n",
        "img, pad = pad_to_square(img, 127.5)\n",
        "padded_h, padded_w, _ = img.shape\n",
        "# Resize to target shape\n",
        "img = lycon.resize(img, height=img_size, width=img_size)\n",
        "# Channels-first and normalize\n",
        "img = torch.from_numpy(img).float().permute((2, 0, 1)) / 255.0\n",
        "\n",
        "# Configure input image\n",
        "img = img[None, :, :, :]  # create a batch of 1 image\n",
        "img = Variable(img.type(Tensor))\n",
        "\n",
        "# Get detections\n",
        "with torch.no_grad():\n",
        "    detections = model(img)\n",
        "    detections_reduced = non_max_suppression(detections, conf_th, iou_th)\n",
        "    print(\"Number of objects after non-max supression: \" + str(len(detections_reduced)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbgOC_Kc7pG4"
      },
      "source": [
        "## Visualize detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z17avoo07qcG"
      },
      "source": [
        "# Create plot\n",
        "img = np.array(Image.open(input_img_filename))\n",
        "plt.figure()\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(img)\n",
        "\n",
        "if detections_reduced is not None:\n",
        "  # Random colors\n",
        "  labels = [d[-1].item() for d in detections_reduced[0]]\n",
        "  unique_labels = np.array(list(set(labels)))\n",
        "  n_cls_preds = unique_labels.shape[0]\n",
        "  import random\n",
        "  cmap = plt.get_cmap(\"tab20b\")\n",
        "  colors = [cmap(i) for i in np.linspace(0, 1, len(detections_reduced))]\n",
        "  bbox_colors = random.sample(colors, n_cls_preds)\n",
        "\n",
        "  # Loop detections\n",
        "  detections_reduced_resized = rescale_boxes(detections_reduced[0], img_size, img.shape[:2])\n",
        "  for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections_reduced_resized:\n",
        "  # Create a Rectangle patch\n",
        "    box_w = x2 - x1\n",
        "    box_h = y2 - y1\n",
        "    color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
        "    bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
        "    # Add the bbox to the plot\n",
        "    ax.add_patch(bbox)\n",
        "    print(x1)\n",
        "    # Add label\n",
        "    plt.text(x1, y1, s=classes[int(cls_pred)], color=\"white\", verticalalignment=\"top\", bbox={\"color\": color, \"pad\": 0},)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E48h4w7Sb_Qn"
      },
      "source": [
        "**Many cats are detected. Fix that using non-max suppression.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKkGdARZ7wfO"
      },
      "source": [
        "## Evaluates mAP on COCO test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF0pwloadHj9"
      },
      "source": [
        "**To save time, I added the result below for you.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgS0KtQk7zQg"
      },
      "source": [
        "%cd ../data/\n",
        "!wget https://raw.githubusercontent.com/heshameraqi/Image-Classification-Object-Detection-with-PyTorch-Workshop/main/data/get_coco_dataset.sh\n",
        "!bash get_coco_dataset.sh\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9F59NYd73wm"
      },
      "source": [
        "%cd PyTorch-YOLOv3\n",
        "!python3 test.py --weights_path ../weights/yolov3.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6oDJPq0dEIE"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/heshameraqi/Image-Classification-Object-Detection-with-PyTorch-Workshop/main/imgs/COCO-AP.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcOGBRqWtvBq"
      },
      "source": [
        "**mAP:**\n",
        "*   detect everything --> perfect recall & zero precision\n",
        "*   detect nothing    --> perfect precision & zero recall\n",
        "*   average precision at different recall values\n",
        "\n",
        "\n"
      ]
    }
  ]
}